{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "import os\n",
        "\n",
        "# ==========================================\n",
        "# 1. PERSIAPAN DATA & DEFINISI MAKNA\n",
        "# ==========================================\n",
        "\n",
        "# Nama file dataset baru Anda\n",
        "file_path = 'dataset_uji_sentimen_variasi_fix.csv'\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Error: File '{file_path}' tidak ditemukan.\")\n",
        "    raise FileNotFoundError(f\"Pastikan file {file_path} sudah ada di direktori yang sama.\")\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Definisi Ground Truth berdasarkan pola kalimat di dataset baru Anda\n",
        "# Kita memetakan contoh-contoh kalimat unik ke label sentimen yang relevan\n",
        "mapping_makna = {\n",
        "    'Layanan ini saya gunakan saat sedang sibuk': 'Netral',\n",
        "    'Saya mengakses aplikasi pada waktu tertentu': 'Netral',\n",
        "    'Saya membuka aplikasi hanya untuk melihat informasi': 'Netral',\n",
        "    'Produk digunakan sesuai dengan fungsinya': 'Positif',\n",
        "    'Produk ini saya beli karena rekomendasi teman': 'Positif',\n",
        "    'Saya menggunakan layanan ini secara rutin': 'Positif',\n",
        "    'Saya memanfaatkan fitur yang tersedia di aplikasi': 'Positif',\n",
        "    'Saya hanya mencoba layanan ini sebentar': 'Netral',\n",
        "    'Aplikasi ini dipakai untuk keperluan sehari-hari': 'Positif',\n",
        "    'Saya menggunakan fitur utama yang tersedia': 'Positif',\n",
        "    'Layanan ini digunakan tanpa kendala berarti': 'Positif',\n",
        "    'Saya mencoba aplikasi ini untuk kebutuhan pribadi': 'Netral',\n",
        "    'Saya mengakses layanan ini melalui ponsel': 'Netral',\n",
        "    'Saya membuka aplikasi untuk mengecek status': 'Netral',\n",
        "    'Saya melakukan transaksi melalui aplikasi tersebut': 'Positif',\n",
        "    'Aplikasi ini digunakan untuk tujuan tertentu': 'Netral',\n",
        "    'Aplikasi ini saya gunakan untuk pertama kali': 'Netral',\n",
        "    'Produk ini saya gunakan sesuai kebutuhan': 'Positif',\n",
        "    # Tambahan jika ada variasi lain yang muncul\n",
        "    'Aplikasi sering mengalami gangguan': 'Negatif',\n",
        "    'Kinerja aplikasi terasa lambat': 'Negatif',\n",
        "    'Saya tidak puas menggunakan aplikasi ini': 'Negatif',\n",
        "    'Tampilan aplikasi membingungkan': 'Negatif'\n",
        "}\n",
        "\n",
        "# Memberikan label awal untuk proses belajar model\n",
        "# Jika kalimat tidak ada di mapping, kita beri label 'Netral' sebagai default\n",
        "df['label'] = df['kalimat'].map(mapping_makna).fillna('Netral')\n",
        "\n",
        "# ==========================================\n",
        "# 2. PREPROCESSING & TOKENISASI\n",
        "# ==========================================\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "df['cleaned_text'] = df['kalimat'].apply(clean_text)\n",
        "\n",
        "# Membangun Kosakata (Vocabulary)\n",
        "all_words = ' '.join(df['cleaned_text']).split()\n",
        "vocab = sorted(list(set(all_words)))\n",
        "word_to_idx = {word: i + 1 for i, word in enumerate(vocab)} # 0 untuk padding\n",
        "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
        "\n",
        "def tokenize(text, max_len=20):\n",
        "    tokens = [word_to_idx[w] for w in text.split() if w in word_to_idx]\n",
        "    if len(tokens) < max_len:\n",
        "        tokens = tokens + [0] * (max_len - len(tokens))\n",
        "    else:\n",
        "        tokens = tokens[:max_len]\n",
        "    return tokens\n",
        "\n",
        "# Konversi ke format numerik\n",
        "X_data = np.array(df['cleaned_text'].apply(lambda x: tokenize(x)).tolist())\n",
        "le = LabelEncoder()\n",
        "y_data = le.fit_transform(df['label'])\n",
        "\n",
        "# ==========================================\n",
        "# 3. ARSITEKTUR MODEL TRANSFORMER\n",
        "# ==========================================\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, num_classes, max_len):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=embed_dim * 4,\n",
        "            dropout=0.1,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, max_len)\n",
        "        x = self.embedding(x) + self.pos_embedding\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=1) # Global Average Pooling\n",
        "        logits = self.fc(x)\n",
        "        return logits\n",
        "\n",
        "# Hyperparameters\n",
        "VOCAB_SIZE = len(vocab) + 1\n",
        "EMBED_DIM = 64\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 2\n",
        "NUM_CLASSES = len(le.classes_)\n",
        "MAX_LEN = 20\n",
        "\n",
        "# ==========================================\n",
        "# 4. PELATIHAN MODEL\n",
        "# ==========================================\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.1, random_state=42)\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.LongTensor(X)\n",
        "        self.y = torch.LongTensor(y)\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_loader = DataLoader(TextDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = TransformerClassifier(VOCAB_SIZE, EMBED_DIM, NUM_HEADS, NUM_LAYERS, NUM_CLASSES, MAX_LEN).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 50\n",
        "print(f\"Memulai pelatihan model pada {device}...\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# ==========================================\n",
        "# 5. EVALUASI & OUTPUT HASIL\n",
        "# ==========================================\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    all_input_tensor = torch.LongTensor(X_data).to(device)\n",
        "    raw_outputs = model(all_input_tensor)\n",
        "    _, predicted_indices = torch.max(raw_outputs, 1)\n",
        "    df['hasil_label_transformer'] = le.inverse_transform(predicted_indices.cpu().numpy())\n",
        "\n",
        "output_file = 'hasil_uji_sentimen_variasi_transformer.csv'\n",
        "df[['kalimat', 'hasil_label_transformer']].to_csv(output_file, index=False)\n",
        "\n",
        "print(\"\\n--- PROSES SELESAI ---\")\n",
        "print(f\"Data diproses: {len(df)} baris\")\n",
        "print(f\"Hasil disimpan di: {output_file}\")\n",
        "print(\"\\nSampel Hasil Prediksi (15 baris pertama):\")\n",
        "print(df[['kalimat', 'hasil_label_transformer']].head(15))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai pelatihan model pada cuda...\n",
            "Epoch [10/50], Loss: 0.0008\n",
            "Epoch [20/50], Loss: 0.0003\n",
            "Epoch [30/50], Loss: 0.0002\n",
            "Epoch [40/50], Loss: 0.0001\n",
            "Epoch [50/50], Loss: 0.0001\n",
            "\n",
            "--- PROSES SELESAI ---\n",
            "Data diproses: 400 baris\n",
            "Hasil disimpan di: hasil_uji_sentimen_variasi_transformer.csv\n",
            "\n",
            "Sampel Hasil Prediksi (15 baris pertama):\n",
            "                                              kalimat hasil_label_transformer\n",
            "0          Layanan ini saya gunakan saat sedang sibuk                  Netral\n",
            "1         Saya mengakses aplikasi pada waktu tertentu                  Netral\n",
            "2   Saya membuka aplikasi hanya untuk melihat info...                  Netral\n",
            "3            Produk digunakan sesuai dengan fungsinya                 Positif\n",
            "4         Saya mengakses aplikasi pada waktu tertentu                  Netral\n",
            "5       Produk ini saya beli karena rekomendasi teman                 Positif\n",
            "6          Layanan ini saya gunakan saat sedang sibuk                  Netral\n",
            "7           Saya menggunakan layanan ini secara rutin                 Positif\n",
            "8   Saya memanfaatkan fitur yang tersedia di aplikasi                 Positif\n",
            "9   Saya memanfaatkan fitur yang tersedia di aplikasi                 Positif\n",
            "10            Saya hanya mencoba layanan ini sebentar                  Netral\n",
            "11            Saya hanya mencoba layanan ini sebentar                  Netral\n",
            "12         Layanan ini saya gunakan saat sedang sibuk                  Netral\n",
            "13   Aplikasi ini dipakai untuk keperluan sehari-hari                 Positif\n",
            "14         Saya menggunakan fitur utama yang tersedia                 Positif\n"
          ]
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcFfIeFY4Th7",
        "outputId": "3f4a9a30-8c82-4544-93ea-8a07450d4b78"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}